# LLM (Ollama) configuration for LLM-derived features (1.10)

connection:
  base_url: "http://localhost:11434"
  model: "llama3:8b"
  timeout_seconds: 60

inference_params:
  temperature: 0.2
  top_p: 0.9
  max_tokens: 512

# Prompt templates for LLM risk analysis
prompts:
  risk_analysis:
    system: |
      You are a digital security and anti-fraud expert.
      You receive JSON with event, user, device, network and message details.
      Your job is to ANALYZE and return a structured JSON with risk assessment only.

    user_template: |
      Analyze the following event and assess fraud and abuse risk.

      Return ONLY a compact JSON with exactly these fields:
      - llm_risk_score (float 0-1)
      - llm_risk_level (one of: low, medium, high, critical)
      - llm_phishing_score (float 0-1)
      - llm_social_engineering_score (float 0-1)
      - llm_urgency_score (float 0-1)
      - llm_sensitivity_request_score (float 0-1)
      - llm_risk_category (one of: financial_fraud, phishing, social_engineering, unauthorized_access, benign, other)
      - llm_fraud_pattern (one of: account_takeover, card_not_present, money_mule, synthetic_identity, none)
      - llm_intent (one of: pay_bill, transfer_to_self, transfer_to_third_party, test_transaction, other)
      - llm_detected_social_engineering_language (0 or 1)
      - llm_detected_request_for_personal_data (0 or 1)
      - llm_detected_threat_or_coercion (0 or 1)
      - llm_detected_suspicious_link (0 or 1)
      - llm_short_explanation (short string)
      - llm_risk_tags (list of short strings/tags)

      Event JSON:
      {{event_json}}

fallback_values:
  llm_risk_score: 0.0
  llm_risk_level: "low"
  llm_phishing_score: 0.0
  llm_social_engineering_score: 0.0
  llm_urgency_score: 0.0
  llm_sensitivity_request_score: 0.0
  llm_risk_category: "benign"
  llm_fraud_pattern: "none"
  llm_intent: "other"
  llm_detected_social_engineering_language: 0
  llm_detected_request_for_personal_data: 0
  llm_detected_threat_or_coercion: 0
  llm_detected_suspicious_link: 0
  llm_short_explanation: "LLM not called"
  llm_risk_tags: []