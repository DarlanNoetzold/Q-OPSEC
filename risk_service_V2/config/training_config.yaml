# ============================================================================
# MODEL TRAINING CONFIGURATION
# ============================================================================

# Data Configuration
data:
  input_dir: "output"
  train_file: "dataset_train.parquet"  # or .csv
  val_file: "dataset_val.parquet"
  test_file: "dataset_test.parquet"

  target_column: "is_fraud"

  # Columns to exclude from features
  exclude_columns:
    - "event_id"
    - "user_id"
    - "account_id"
    - "is_fraud"
    - "fraud_type"
    - "timestamp_utc"
    - "timestamp_local"
    - "account_creation_date"
    - "message_text"
    - "llm_risk_reasoning"
    - "transaction_description"
    - "ip_address"
    - "device_id"

  # Feature types (auto-detected if not specified)
  numeric_features: []  # Auto-detect if empty
  categorical_features: []  # Auto-detect if empty

  # Missing value handling
  missing_value_strategy:
    numeric: "median"  # mean, median, zero, drop
    categorical: "mode"  # mode, unknown, drop

  # Class balancing
  balance_classes: false
  balance_method: "none"  # none, smote, undersample, oversample
  balance_ratio: 0.5  # Target ratio for minority class

# Feature Engineering
feature_engineering:
  # Categorical encoding
  categorical_encoding: "label"  # label, onehot, target
  max_categories_onehot: 10  # Max unique values for one-hot encoding

  # Numeric scaling
  numeric_scaling: "standard"  # standard, minmax, robust, none

  # Feature selection
  feature_selection:
    enabled: false
    method: "importance"  # importance, correlation, mutual_info
    top_k: 50  # Number of features to keep
    correlation_threshold: 0.95  # Remove highly correlated features

# Model Configuration
models:
  enabled:
    - xgboost
    - lightgbm
    - logistic_regression
    - random_forest
    - catboost
    - pytorch_mlp

  catboost:
    params:
      iterations: 1000
      learning_rate: 0.05
      depth: 6
      loss_function: Logloss
      eval_metric: AUC
      random_seed: 42
      verbose: False

  pytorch_mlp:
    epochs: 20
    batch_size: 64
    learning_rate: 0.001

  # XGBoost
  xgboost:
    params:
      objective: "binary:logistic"
      eval_metric: "auc"
      max_depth: 6
      learning_rate: 0.1
      n_estimators: 100
      subsample: 0.8
      colsample_bytree: 0.8
      min_child_weight: 1
      gamma: 0
      reg_alpha: 0
      reg_lambda: 1
      scale_pos_weight: 1  # Auto-calculated if null
      random_state: 42
      n_jobs: -1
      tree_method: "hist"

    hyperparameter_tuning:
      enabled: false
      method: "random"  # grid, random, optuna
      n_trials: 50
      cv_folds: 3
      param_grid:
        max_depth: [3, 5, 7, 9]
        learning_rate: [0.01, 0.05, 0.1, 0.2]
        n_estimators: [50, 100, 200, 300]
        subsample: [0.6, 0.8, 1.0]
        colsample_bytree: [0.6, 0.8, 1.0]
        min_child_weight: [1, 3, 5]
        gamma: [0, 0.1, 0.2]

  # LightGBM
  lightgbm:
    params:
      objective: "binary"
      metric: "auc"
      boosting_type: "gbdt"
      num_leaves: 31
      learning_rate: 0.1
      n_estimators: 100
      subsample: 0.8
      colsample_bytree: 0.8
      min_child_samples: 20
      reg_alpha: 0
      reg_lambda: 1
      scale_pos_weight: 1
      random_state: 42
      n_jobs: -1
      verbose: -1

    hyperparameter_tuning:
      enabled: false
      method: "random"
      n_trials: 50
      cv_folds: 3
      param_grid:
        num_leaves: [15, 31, 63, 127]
        learning_rate: [0.01, 0.05, 0.1, 0.2]
        n_estimators: [50, 100, 200, 300]
        subsample: [0.6, 0.8, 1.0]
        colsample_bytree: [0.6, 0.8, 1.0]
        min_child_samples: [10, 20, 30]

  # Random Forest
  random_forest:
    params:
      n_estimators: 100
      max_depth: 10
      min_samples_split: 2
      min_samples_leaf: 1
      max_features: "sqrt"
      bootstrap: true
      class_weight: "balanced"
      random_state: 42
      n_jobs: -1
      verbose: 0

    hyperparameter_tuning:
      enabled: false
      method: "random"
      n_trials: 30
      cv_folds: 3
      param_grid:
        n_estimators: [50, 100, 200]
        max_depth: [5, 10, 15, 20, null]
        min_samples_split: [2, 5, 10]
        min_samples_leaf: [1, 2, 4]
        max_features: ["sqrt", "log2"]

  # Logistic Regression (Baseline)
  logistic_regression:
    params:
      penalty: "l2"
      C: 1.0
      solver: "lbfgs"
      max_iter: 1000
      class_weight: "balanced"
      random_state: 42
      n_jobs: -1
      verbose: 0

    hyperparameter_tuning:
      enabled: false
      method: "grid"
      cv_folds: 3
      param_grid:
        C: [0.001, 0.01, 0.1, 1, 10, 100]
        penalty: ["l1", "l2"]
        solver: ["liblinear", "saga"]

# Training Configuration
training:
  # Cross-validation
  cross_validation:
    enabled: false
    n_folds: 5
    stratified: true

  # Early stopping (for boosting models)
  early_stopping:
    enabled: true
    rounds: 50
    metric: "auc"

  # Evaluation metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
    - "pr_auc"
    - "log_loss"

  # Threshold optimization
  threshold_optimization:
    enabled: true
    metric: "f1"  # f1, precision, recall, youden
    search_range: [0.1, 0.9]
    search_steps: 81

# Evaluation Configuration
evaluation:
  # Generate visualizations
  visualizations:
    enabled: true
    plots:
      - "confusion_matrix"
      - "roc_curve"
      - "precision_recall_curve"
      - "feature_importance"
      - "calibration_curve"
      - "threshold_analysis"

  # Feature importance
  feature_importance:
    enabled: true
    top_n: 30
    method: "gain"  # gain, weight, cover (for tree models)

  # SHAP analysis (expensive, optional)
  shap_analysis:
    enabled: false
    sample_size: 1000  # Number of samples for SHAP
    plot_types:
      - "summary"
      - "waterfall"
      - "dependence"

# Output Configuration
output:
  models_dir: "output/models"
  evaluation_dir: "output/evaluation"
  logs_dir: "logs"

  # Model versioning
  versioning:
    enabled: true
    format: "timestamp"  # timestamp, sequential, semantic

  # Save artifacts
  save_artifacts:
    model: true
    preprocessor: true
    feature_names: true
    metrics: true
    predictions: true
    config: true

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  console: true
  file: true
  file_name: "training.log"