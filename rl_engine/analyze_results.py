import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import sys
from scipy import stats

# Style configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10


class ResultAnalyzer:
    def __init__(self, json_file: str):
        """Initialize analyzer with JSON results file"""
        with open(json_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.df = pd.DataFrame(self.data['raw_results'])
        self.output_dir = Path('analysis_output')
        self.output_dir.mkdir(exist_ok=True)

    def generate_all_plots(self):
        """Generate all plots for the article"""
        print("\nüìä Generating visualizations...")

        self.plot_algorithm_distribution()
        self.plot_success_rate_by_security()
        self.plot_latency_comparison()
        self.plot_qkd_vs_non_qkd()
        self.plot_response_time_distribution()
        self.plot_security_level_heatmap()
        self.plot_temporal_analysis()
        self.plot_resource_usage()

        # NEW PLOTS
        self.plot_algorithm_category_comparison()
        self.plot_learning_curve()
        self.plot_risk_vs_performance()
        self.plot_algorithm_success_matrix()
        self.plot_latency_vs_security_scatter()
        self.plot_episode_progression()
        self.plot_algorithm_selection_evolution()
        self.plot_performance_radar()
        self.plot_correlation_matrix()
        self.plot_cumulative_success()

        print(f"\n‚úÖ Plots saved in: {self.output_dir}/")

    def plot_algorithm_distribution(self):
        """Algorithm usage distribution"""
        algo_counts = {}
        for _, row in self.df.iterrows():
            for algo in row['proposed_algorithms']:
                algo_counts[algo] = algo_counts.get(algo, 0) + 1

        fig, ax = plt.subplots(figsize=(14, 8))
        algos = list(algo_counts.keys())
        counts = list(algo_counts.values())

        colors = plt.cm.viridis(np.linspace(0, 1, len(algos)))
        bars = ax.barh(algos, counts, color=colors)

        ax.set_xlabel('Number of Times Selected', fontsize=12, fontweight='bold')
        ax.set_ylabel('Cryptographic Algorithm', fontsize=12, fontweight='bold')
        ax.set_title('Distribution of Cryptographic Algorithm Usage',
                     fontsize=14, fontweight='bold', pad=20)

        # Add values on bars
        for bar in bars:
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height() / 2,
                    f'{int(width)}', ha='left', va='center', fontsize=10)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'algorithm_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì algorithm_distribution.png")

    def plot_success_rate_by_security(self):
        """Success rate by security level"""
        success_by_level = self.df.groupby('security_level').agg({
            'feedback_success': ['mean', 'count']
        }).reset_index()

        success_by_level.columns = ['security_level', 'success_rate', 'count']
        success_by_level['success_rate'] *= 100

        fig, ax = plt.subplots(figsize=(12, 7))

        bars = ax.bar(success_by_level['security_level'],
                      success_by_level['success_rate'],
                      color=plt.cm.RdYlGn(success_by_level['success_rate'] / 100))

        ax.set_xlabel('Security Level', fontsize=12, fontweight='bold')
        ax.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')
        ax.set_title('Success Rate by Security Level',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_ylim(0, 105)

        # Add values and count
        for i, bar in enumerate(bars):
            height = bar.get_height()
            count = success_by_level.iloc[i]['count']
            ax.text(bar.get_x() + bar.get_width() / 2, height + 1,
                    f'{height:.1f}%\n(n={count})',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'success_rate_by_security.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì success_rate_by_security.png")

    def plot_latency_comparison(self):
        """Latency comparison between security levels"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

        # Box plot
        self.df.boxplot(column='feedback_latency', by='security_level', ax=ax1)
        ax1.set_xlabel('Security Level', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Latency (ms)', fontsize=12, fontweight='bold')
        ax1.set_title('Latency Distribution by Security Level',
                      fontsize=12, fontweight='bold')
        plt.sca(ax1)
        plt.xticks(rotation=45, ha='right')

        # Violin plot
        sns.violinplot(data=self.df, x='security_level', y='feedback_latency', ax=ax2)
        ax2.set_xlabel('Security Level', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Latency (ms)', fontsize=12, fontweight='bold')
        ax2.set_title('Latency Density by Security Level',
                      fontsize=12, fontweight='bold')
        plt.sca(ax2)
        plt.xticks(rotation=45, ha='right')

        plt.suptitle('')  # Remove automatic title
        plt.tight_layout()
        plt.savefig(self.output_dir / 'latency_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì latency_comparison.png")

    def plot_qkd_vs_non_qkd(self):
        """QKD vs Non-QKD comparison"""
        qkd_comparison = self.df.groupby('has_qkd').agg({
            'feedback_success': 'mean',
            'feedback_latency': 'mean',
            'feedback_resource_usage': 'mean',
            'request_id': 'count'
        }).reset_index()

        qkd_comparison.columns = ['has_qkd', 'success_rate', 'avg_latency',
                                  'avg_resource', 'count']
        qkd_comparison['success_rate'] *= 100
        qkd_comparison['has_qkd'] = qkd_comparison['has_qkd'].map({
            True: 'With QKD', False: 'Without QKD'
        })

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Success rate
        axes[0, 0].bar(qkd_comparison['has_qkd'], qkd_comparison['success_rate'],
                       color=['#2ecc71', '#e74c3c'])
        axes[0, 0].set_ylabel('Success Rate (%)', fontweight='bold')
        axes[0, 0].set_title('Success Rate', fontweight='bold')
        axes[0, 0].set_ylim(0, 105)
        for i, v in enumerate(qkd_comparison['success_rate']):
            axes[0, 0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')

        # Latency
        axes[0, 1].bar(qkd_comparison['has_qkd'], qkd_comparison['avg_latency'],
                       color=['#3498db', '#9b59b6'])
        axes[0, 1].set_ylabel('Average Latency (ms)', fontweight='bold')
        axes[0, 1].set_title('Average Latency', fontweight='bold')
        for i, v in enumerate(qkd_comparison['avg_latency']):
            axes[0, 1].text(i, v + 2, f'{v:.1f}ms', ha='center', fontweight='bold')

        # Resource usage
        axes[1, 0].bar(qkd_comparison['has_qkd'], qkd_comparison['avg_resource'],
                       color=['#f39c12', '#1abc9c'])
        axes[1, 0].set_ylabel('Average Resource Usage', fontweight='bold')
        axes[1, 0].set_title('Resource Usage', fontweight='bold')
        axes[1, 0].set_ylim(0, 1.1)
        for i, v in enumerate(qkd_comparison['avg_resource']):
            axes[1, 0].text(i, v + 0.05, f'{v:.2f}', ha='center', fontweight='bold')

        # Count
        axes[1, 1].bar(qkd_comparison['has_qkd'], qkd_comparison['count'],
                       color=['#34495e', '#95a5a6'])
        axes[1, 1].set_ylabel('Number of Requests', fontweight='bold')
        axes[1, 1].set_title('Request Volume', fontweight='bold')
        for i, v in enumerate(qkd_comparison['count']):
            axes[1, 1].text(i, v + 5, f'{int(v)}', ha='center', fontweight='bold')

        plt.suptitle('Comparative Analysis: QKD vs Non-QKD',
                     fontsize=16, fontweight='bold', y=1.00)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'qkd_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì qkd_comparison.png")

    def plot_response_time_distribution(self):
        """Response time distribution"""
        fig, ax = plt.subplots(figsize=(12, 7))

        ax.hist(self.df['response_time'], bins=30, color='skyblue',
                edgecolor='black', alpha=0.7)
        ax.axvline(self.df['response_time'].mean(), color='red',
                   linestyle='--', linewidth=2, label=f'Mean: {self.df["response_time"].mean():.4f}s')
        ax.axvline(self.df['response_time'].median(), color='green',
                   linestyle='--', linewidth=2, label=f'Median: {self.df["response_time"].median():.4f}s')

        ax.set_xlabel('Response Time (seconds)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')
        ax.set_title('RL Engine Response Time Distribution',
                     fontsize=14, fontweight='bold', pad=20)
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'response_time_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì response_time_distribution.png")

    def plot_security_level_heatmap(self):
        """Metrics heatmap by security level"""
        metrics_by_level = self.df.groupby('security_level').agg({
            'feedback_success': 'mean',
            'feedback_latency': 'mean',
            'feedback_resource_usage': 'mean',
            'response_time': 'mean'
        })

        # Normalize to 0-1
        metrics_normalized = (metrics_by_level - metrics_by_level.min()) / (
                metrics_by_level.max() - metrics_by_level.min())

        fig, ax = plt.subplots(figsize=(10, 8))

        sns.heatmap(metrics_normalized.T, annot=True, fmt='.2f', cmap='RdYlGn',
                    cbar_kws={'label': 'Normalized Value (0-1)'}, ax=ax)

        ax.set_xlabel('Security Level', fontsize=12, fontweight='bold')
        ax.set_ylabel('Metric', fontsize=12, fontweight='bold')
        ax.set_title('Metrics Heatmap by Security Level (Normalized)',
                     fontsize=14, fontweight='bold', pad=20)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'security_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì security_heatmap.png")

    def plot_temporal_analysis(self):
        """Temporal analysis of results"""
        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])
        self.df = self.df.sort_values('timestamp')
        self.df['request_number'] = range(len(self.df))

        fig, axes = plt.subplots(3, 1, figsize=(14, 12))

        # Success rate over time
        window = 20
        self.df['success_rolling'] = self.df['feedback_success'].rolling(window=window).mean() * 100
        axes[0].plot(self.df['request_number'], self.df['success_rolling'],
                     linewidth=2, color='green')
        axes[0].set_ylabel('Success Rate (%) - Moving Average', fontweight='bold')
        axes[0].set_title(f'Success Rate Evolution (window={window})', fontweight='bold')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_ylim(0, 105)

        # Latency over time
        self.df['latency_rolling'] = self.df['feedback_latency'].rolling(window=window).mean()
        axes[1].plot(self.df['request_number'], self.df['latency_rolling'],
                     linewidth=2, color='blue')
        axes[1].set_ylabel('Latency (ms) - Moving Average', fontweight='bold')
        axes[1].set_title(f'Latency Evolution (window={window})', fontweight='bold')
        axes[1].grid(True, alpha=0.3)

        # Resource usage over time
        self.df['resource_rolling'] = self.df['feedback_resource_usage'].rolling(window=window).mean()
        axes[2].plot(self.df['request_number'], self.df['resource_rolling'],
                     linewidth=2, color='orange')
        axes[2].set_xlabel('Request Number', fontweight='bold')
        axes[2].set_ylabel('Resource Usage - Moving Average', fontweight='bold')
        axes[2].set_title(f'Resource Usage Evolution (window={window})', fontweight='bold')
        axes[2].grid(True, alpha=0.3)
        axes[2].set_ylim(0, 1.1)

        plt.suptitle('Temporal Performance Analysis', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'temporal_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì temporal_analysis.png")

    def plot_resource_usage(self):
        """Resource usage analysis"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # Scatter: Resources vs Latency
        scatter = axes[0].scatter(self.df['feedback_resource_usage'],
                                  self.df['feedback_latency'],
                                  c=self.df['feedback_success'].astype(int),
                                  cmap='RdYlGn', alpha=0.6, s=50)
        axes[0].set_xlabel('Resource Usage', fontweight='bold')
        axes[0].set_ylabel('Latency (ms)', fontweight='bold')
        axes[0].set_title('Relationship: Resource Usage vs Latency', fontweight='bold')
        axes[0].grid(True, alpha=0.3)
        cbar = plt.colorbar(scatter, ax=axes[0])
        cbar.set_label('Success', fontweight='bold')

        # Resource usage distribution
        axes[1].hist(self.df['feedback_resource_usage'], bins=20,
                     color='coral', edgecolor='black', alpha=0.7)
        axes[1].axvline(self.df['feedback_resource_usage'].mean(),
                        color='red', linestyle='--', linewidth=2,
                        label=f'Mean: {self.df["feedback_resource_usage"].mean():.2f}')
        axes[1].set_xlabel('Resource Usage', fontweight='bold')
        axes[1].set_ylabel('Frequency', fontweight='bold')
        axes[1].set_title('Resource Usage Distribution', fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'resource_usage_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì resource_usage_analysis.png")

    def plot_algorithm_category_comparison(self):
        """Compare algorithm categories (QKD, PQC, Hybrid, Classical)"""

        # Categorize algorithms
        def categorize_algo(algo):
            if 'QKD' in algo and 'HYBRID' not in algo:
                return 'QKD'
            elif 'PQC' in algo and 'HYBRID' not in algo:
                return 'PQC'
            elif 'HYBRID' in algo:
                return 'Hybrid'
            else:
                return 'Classical'

        # Create category column
        algo_categories = []
        for _, row in self.df.iterrows():
            for algo in row['proposed_algorithms']:
                algo_categories.append({
                    'category': categorize_algo(algo),
                    'success': row['feedback_success'],
                    'latency': row['feedback_latency'],
                    'resource': row['feedback_resource_usage']
                })

        cat_df = pd.DataFrame(algo_categories)

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Success rate by category
        success_by_cat = cat_df.groupby('category')['success'].mean() * 100
        axes[0, 0].bar(success_by_cat.index, success_by_cat.values,
                       color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
        axes[0, 0].set_ylabel('Success Rate (%)', fontweight='bold')
        axes[0, 0].set_title('Success Rate by Algorithm Category', fontweight='bold')
        axes[0, 0].set_ylim(0, 105)
        for i, v in enumerate(success_by_cat.values):
            axes[0, 0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')

        # Latency by category
        sns.boxplot(data=cat_df, x='category', y='latency', ax=axes[0, 1])
        axes[0, 1].set_xlabel('Algorithm Category', fontweight='bold')
        axes[0, 1].set_ylabel('Latency (ms)', fontweight='bold')
        axes[0, 1].set_title('Latency Distribution by Category', fontweight='bold')

        # Resource usage by category
        sns.violinplot(data=cat_df, x='category', y='resource', ax=axes[1, 0])
        axes[1, 0].set_xlabel('Algorithm Category', fontweight='bold')
        axes[1, 0].set_ylabel('Resource Usage', fontweight='bold')
        axes[1, 0].set_title('Resource Usage by Category', fontweight='bold')

        # Count by category
        count_by_cat = cat_df['category'].value_counts()
        axes[1, 1].pie(count_by_cat.values, labels=count_by_cat.index,
                       autopct='%1.1f%%', startangle=90,
                       colors=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])
        axes[1, 1].set_title('Algorithm Category Distribution', fontweight='bold')

        plt.suptitle('Algorithm Category Comparison', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'algorithm_category_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì algorithm_category_comparison.png")

    def plot_learning_curve(self):
        """Learning curve showing improvement over episodes"""
        if 'metrics_history' not in self.data or not self.data['metrics_history']:
            print("  ‚ö† Skipping learning_curve.png (no episode data)")
            return

        episodes = []
        for metric in self.data['metrics_history']:
            episodes.append({
                'episode': metric.get('episode', 0),
                'total_reward': metric.get('total_reward', 0),
                'avg_reward': metric.get('average_reward', 0),
                'epsilon': metric.get('epsilon', 0)
            })

        ep_df = pd.DataFrame(episodes)

        fig, axes = plt.subplots(2, 1, figsize=(12, 10))

        # Total reward per episode
        axes[0].plot(ep_df['episode'], ep_df['total_reward'],
                     marker='o', linewidth=2, markersize=6, color='blue')
        axes[0].set_xlabel('Episode', fontweight='bold')
        axes[0].set_ylabel('Total Reward', fontweight='bold')
        axes[0].set_title('Total Reward per Episode', fontweight='bold')
        axes[0].grid(True, alpha=0.3)

        # Average reward and epsilon
        ax2 = axes[1]
        ax2.plot(ep_df['episode'], ep_df['avg_reward'],
                 marker='o', linewidth=2, markersize=6, color='green', label='Avg Reward')
        ax2.set_xlabel('Episode', fontweight='bold')
        ax2.set_ylabel('Average Reward', fontweight='bold', color='green')
        ax2.tick_params(axis='y', labelcolor='green')
        ax2.grid(True, alpha=0.3)

        ax3 = ax2.twinx()
        ax3.plot(ep_df['episode'], ep_df['epsilon'],
                 marker='s', linewidth=2, markersize=6, color='red', label='Epsilon')
        ax3.set_ylabel('Epsilon (Exploration Rate)', fontweight='bold', color='red')
        ax3.tick_params(axis='y', labelcolor='red')

        axes[1].set_title('Learning Progress: Reward and Exploration', fontweight='bold')

        plt.suptitle('RL Engine Learning Curve', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'learning_curve.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì learning_curve.png")

    def plot_risk_vs_performance(self):
        """Risk score vs performance metrics"""
        if 'risk_score' not in self.df.columns:
            print("  ‚ö† Skipping risk_vs_performance.png (no risk_score data)")
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        # Risk vs Success
        risk_bins = pd.cut(self.df['risk_score'], bins=5)
        success_by_risk = self.df.groupby(risk_bins)['feedback_success'].mean() * 100
        axes[0].plot(range(len(success_by_risk)), success_by_risk.values,
                     marker='o', linewidth=2, markersize=8, color='green')
        axes[0].set_xlabel('Risk Score Range', fontweight='bold')
        axes[0].set_ylabel('Success Rate (%)', fontweight='bold')
        axes[0].set_title('Success Rate vs Risk Score', fontweight='bold')
        axes[0].set_xticks(range(len(success_by_risk)))
        axes[0].set_xticklabels([f'{i.left:.2f}-{i.right:.2f}' for i in success_by_risk.index], rotation=45)
        axes[0].grid(True, alpha=0.3)

        # Risk vs Latency
        scatter1 = axes[1].scatter(self.df['risk_score'], self.df['feedback_latency'],
                                   c=self.df['feedback_success'].astype(int),
                                   cmap='RdYlGn', alpha=0.6, s=50)
        axes[1].set_xlabel('Risk Score', fontweight='bold')
        axes[1].set_ylabel('Latency (ms)', fontweight='bold')
        axes[1].set_title('Latency vs Risk Score', fontweight='bold')
        axes[1].grid(True, alpha=0.3)
        plt.colorbar(scatter1, ax=axes[1], label='Success')

        # Risk vs Resource
        scatter2 = axes[2].scatter(self.df['risk_score'], self.df['feedback_resource_usage'],
                                   c=self.df['security_level'].astype('category').cat.codes,
                                   cmap='viridis', alpha=0.6, s=50)
        axes[2].set_xlabel('Risk Score', fontweight='bold')
        axes[2].set_ylabel('Resource Usage', fontweight='bold')
        axes[2].set_title('Resource Usage vs Risk Score', fontweight='bold')
        axes[2].grid(True, alpha=0.3)
        plt.colorbar(scatter2, ax=axes[2], label='Security Level')

        plt.suptitle('Risk Score vs Performance Metrics', fontsize=16, fontweight='bold', y=1.00)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'risk_vs_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì risk_vs_performance.png")

    def plot_algorithm_success_matrix(self):
        """Success rate matrix for each algorithm"""
        # Get expected algorithms and their success
        algo_success = {}
        for _, row in self.df.iterrows():
            expected = row['expected_algorithm']
            success = row['feedback_success']
            if expected not in algo_success:
                algo_success[expected] = []
            algo_success[expected].append(success)

        # Calculate success rate for each algorithm
        algo_stats = {}
        for algo, successes in algo_success.items():
            algo_stats[algo] = {
                'success_rate': np.mean(successes) * 100,
                'count': len(successes),
                'std': np.std(successes) * 100
            }

        # Sort by success rate
        sorted_algos = sorted(algo_stats.items(), key=lambda x: x[1]['success_rate'], reverse=True)

        fig, ax = plt.subplots(figsize=(14, 10))

        algos = [a[0] for a in sorted_algos]
        success_rates = [a[1]['success_rate'] for a in sorted_algos]
        counts = [a[1]['count'] for a in sorted_algos]
        stds = [a[1]['std'] for a in sorted_algos]

        colors = plt.cm.RdYlGn(np.array(success_rates) / 100)
        bars = ax.barh(algos, success_rates, color=colors, xerr=stds, capsize=5)

        ax.set_xlabel('Success Rate (%)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Expected Algorithm', fontsize=12, fontweight='bold')
        ax.set_title('Algorithm Success Rate Matrix (with Standard Deviation)',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_xlim(0, 105)

        # Add values and counts
        for i, (bar, count) in enumerate(zip(bars, counts)):
            width = bar.get_width()
            ax.text(width + 2, bar.get_y() + bar.get_height() / 2,
                    f'{width:.1f}% (n={count})',
                    ha='left', va='center', fontsize=9, fontweight='bold')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'algorithm_success_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì algorithm_success_matrix.png")

    def plot_latency_vs_security_scatter(self):
        """3D-like scatter: Latency vs Security vs Success"""
        fig, ax = plt.subplots(figsize=(14, 8))

        # Map security levels to numeric values
        security_map = {'moderate': 1, 'high': 2, 'very_high': 3, 'ultra': 4}
        self.df['security_numeric'] = self.df['security_level'].map(security_map)

        scatter = ax.scatter(self.df['security_numeric'],
                             self.df['feedback_latency'],
                             s=self.df['feedback_resource_usage'] * 500,
                             c=self.df['feedback_success'].astype(int),
                             cmap='RdYlGn', alpha=0.6, edgecolors='black', linewidth=0.5)

        ax.set_xlabel('Security Level', fontsize=12, fontweight='bold')
        ax.set_ylabel('Latency (ms)', fontsize=12, fontweight='bold')
        ax.set_title('Latency vs Security Level (bubble size = resource usage)',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(list(security_map.values()))
        ax.set_xticklabels(list(security_map.keys()), rotation=45, ha='right')
        ax.grid(True, alpha=0.3)

        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Success', fontweight='bold')

        # Add legend for bubble size
        for size in [0.3, 0.6, 0.9]:
            ax.scatter([], [], s=size * 500, c='gray', alpha=0.6,
                       edgecolors='black', linewidth=0.5,
                       label=f'Resource: {size:.1f}')
        ax.legend(scatterpoints=1, frameon=True, labelspacing=2, title='Resource Usage')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'latency_vs_security_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì latency_vs_security_scatter.png")

    def plot_episode_progression(self):
        """Show progression across episodes"""
        if 'metrics_history' not in self.data or not self.data['metrics_history']:
            print("  ‚ö† Skipping episode_progression.png (no episode data)")
            return

        # Extract episode metrics
        episodes_data = []
        for i, metric in enumerate(self.data['metrics_history'], 1):
            episodes_data.append({
                'episode': i,
                'total_reward': metric.get('total_reward', 0),
                'avg_reward': metric.get('average_reward', 0),
                'epsilon': metric.get('epsilon', 0),
                'loss': metric.get('loss', 0)
            })

        ep_df = pd.DataFrame(episodes_data)

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Total reward
        axes[0, 0].plot(ep_df['episode'], ep_df['total_reward'],
                        marker='o', linewidth=2, color='blue')
        axes[0, 0].fill_between(ep_df['episode'], 0, ep_df['total_reward'], alpha=0.3)
        axes[0, 0].set_xlabel('Episode', fontweight='bold')
        axes[0, 0].set_ylabel('Total Reward', fontweight='bold')
        axes[0, 0].set_title('Total Reward Progression', fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3)

        # Average reward
        axes[0, 1].plot(ep_df['episode'], ep_df['avg_reward'],
                        marker='s', linewidth=2, color='green')
        axes[0, 1].set_xlabel('Episode', fontweight='bold')
        axes[0, 1].set_ylabel('Average Reward', fontweight='bold')
        axes[0, 1].set_title('Average Reward Progression', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)

        # Epsilon decay
        axes[1, 0].plot(ep_df['episode'], ep_df['epsilon'],
                        marker='^', linewidth=2, color='red')
        axes[1, 0].set_xlabel('Episode', fontweight='bold')
        axes[1, 0].set_ylabel('Epsilon', fontweight='bold')
        axes[1, 0].set_title('Exploration Rate Decay', fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3)

        # Loss
        if ep_df['loss'].sum() > 0:
            axes[1, 1].plot(ep_df['episode'], ep_df['loss'],
                            marker='d', linewidth=2, color='orange')
            axes[1, 1].set_xlabel('Episode', fontweight='bold')
            axes[1, 1].set_ylabel('Loss', fontweight='bold')
            axes[1, 1].set_title('Training Loss', fontweight='bold')
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'No Loss Data Available',
                            ha='center', va='center', fontsize=14)
            axes[1, 1].axis('off')

        plt.suptitle('Episode-by-Episode Progression', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'episode_progression.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì episode_progression.png")

    def plot_algorithm_selection_evolution(self):
        """Evolution of algorithm selection over time"""
        # Get top 10 most used algorithms
        algo_counts = {}
        for _, row in self.df.iterrows():
            for algo in row['proposed_algorithms']:
                algo_counts[algo] = algo_counts.get(algo, 0) + 1

        top_algos = sorted(algo_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        top_algo_names = [a[0] for a in top_algos]

        # Track usage over time (in windows)
        window_size = len(self.df) // 10
        windows = []

        for i in range(0, len(self.df), window_size):
            window_df = self.df.iloc[i:i + window_size]
            window_counts = {algo: 0 for algo in top_algo_names}

            for _, row in window_df.iterrows():
                for algo in row['proposed_algorithms']:
                    if algo in window_counts:
                        window_counts[algo] += 1

            windows.append(window_counts)

        # Create stacked area chart
        fig, ax = plt.subplots(figsize=(14, 8))

        window_numbers = list(range(len(windows)))
        bottom = np.zeros(len(windows))

        colors = plt.cm.tab20(np.linspace(0, 1, len(top_algo_names)))

        for i, algo in enumerate(top_algo_names):
            values = [w[algo] for w in windows]
            ax.fill_between(window_numbers, bottom, bottom + values,
                            label=algo, alpha=0.7, color=colors[i])
            bottom += values

        ax.set_xlabel('Time Window', fontsize=12, fontweight='bold')
        ax.set_ylabel('Algorithm Usage Count', fontsize=12, fontweight='bold')
        ax.set_title('Algorithm Selection Evolution Over Time (Top 10)',
                     fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=9)
        ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'algorithm_selection_evolution.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì algorithm_selection_evolution.png")

    def plot_performance_radar(self):
        """Radar chart comparing performance across security levels"""
        # Calculate metrics by security level
        security_levels = self.df['security_level'].unique()
        metrics = ['feedback_success', 'feedback_latency', 'feedback_resource_usage', 'response_time']

        data_by_level = {}
        for level in security_levels:
            level_df = self.df[self.df['security_level'] == level]
            data_by_level[level] = {
                'success': level_df['feedback_success'].mean() * 100,
                'latency': 100 - (level_df['feedback_latency'].mean() / self.df['feedback_latency'].max() * 100),
                'resource': 100 - (level_df['feedback_resource_usage'].mean() * 100),
                'response': 100 - (level_df['response_time'].mean() / self.df['response_time'].max() * 100)
            }

        # Create radar chart
        categories = ['Success\nRate', 'Low\nLatency', 'Low\nResource', 'Fast\nResponse']
        N = len(categories)

        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

        colors = plt.cm.viridis(np.linspace(0, 1, len(security_levels)))

        for i, (level, data) in enumerate(data_by_level.items()):
            values = [data['success'], data['latency'], data['resource'], data['response']]
            values += values[:1]

            ax.plot(angles, values, 'o-', linewidth=2, label=level, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=11, fontweight='bold')
        ax.set_ylim(0, 100)
        ax.set_yticks([20, 40, 60, 80, 100])
        ax.set_yticklabels(['20', '40', '60', '80', '100'], fontsize=9)
        ax.grid(True)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)

        plt.title('Performance Radar Chart by Security Level\n(Higher is Better)',
                  fontsize=14, fontweight='bold', pad=20)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'performance_radar.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì performance_radar.png")

    def plot_correlation_matrix(self):
        """Correlation matrix of numerical features"""
        # Select numerical columns
        numerical_cols = ['feedback_success', 'feedback_latency',
                          'feedback_resource_usage', 'response_time']

        if 'risk_score' in self.df.columns:
            numerical_cols.append('risk_score')
        if 'conf_score' in self.df.columns:
            numerical_cols.append('conf_score')
        if 'data_sensitivity' in self.df.columns:
            numerical_cols.append('data_sensitivity')

        corr_df = self.df[numerical_cols].corr()

        fig, ax = plt.subplots(figsize=(10, 8))

        sns.heatmap(corr_df, annot=True, fmt='.2f', cmap='coolwarm',
                    center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)

        ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì correlation_matrix.png")

    def plot_cumulative_success(self):
        """Cumulative success rate over time"""
        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])
        self.df = self.df.sort_values('timestamp')
        self.df['cumulative_success'] = self.df['feedback_success'].cumsum()
        self.df['cumulative_total'] = range(1, len(self.df) + 1)
        self.df['cumulative_rate'] = (self.df['cumulative_success'] / self.df['cumulative_total']) * 100

        fig, ax = plt.subplots(figsize=(14, 7))

        ax.plot(self.df['cumulative_total'], self.df['cumulative_rate'],
                linewidth=2, color='blue')
        ax.fill_between(self.df['cumulative_total'], 0, self.df['cumulative_rate'],
                        alpha=0.3, color='blue')

        # Add target line
        ax.axhline(y=90, color='green', linestyle='--', linewidth=2, label='90% Target')
        ax.axhline(y=95, color='orange', linestyle='--', linewidth=2, label='95% Target')

        ax.set_xlabel('Number of Requests', fontsize=12, fontweight='bold')
        ax.set_ylabel('Cumulative Success Rate (%)', fontsize=12, fontweight='bold')
        ax.set_title('Cumulative Success Rate Over Time',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_ylim(0, 105)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=11)

        # Add final value annotation
        final_rate = self.df['cumulative_rate'].iloc[-1]
        ax.annotate(f'Final: {final_rate:.2f}%',
                    xy=(len(self.df), final_rate),
                    xytext=(len(self.df) * 0.7, final_rate - 10),
                    arrowprops=dict(arrowstyle='->', color='red', lw=2),
                    fontsize=12, fontweight='bold', color='red')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'cumulative_success.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ‚úì cumulative_success.png")

    def generate_latex_tables(self):
        """Generate tables in LaTeX format for the article"""
        print("\nüìù Generating LaTeX tables...")

        latex_file = self.output_dir / 'tables.tex'

        with open(latex_file, 'w', encoding='utf-8') as f:
            # Table 1: General metrics
            f.write("% Table 1: General Performance Metrics\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{General Performance Metrics of RL Engine}\n")
            f.write("\\begin{tabular}{|l|r|}\n")
            f.write("\\hline\n")
            f.write("\\textbf{Metric} & \\textbf{Value} \\\\\n")
            f.write("\\hline\n")

            perf = self.data['performance_metrics']
            f.write(f"Success Rate & {perf['success_rate']:.2f}\\% \\\\\n")
            f.write(f"Average Latency & {perf['avg_latency_ms']:.2f} ms \\\\\n")
            f.write(f"Latency Std Dev & {perf['std_latency_ms']:.2f} ms \\\\\n")
            f.write(f"Minimum Latency & {perf['min_latency_ms']:.2f} ms \\\\\n")
            f.write(f"Maximum Latency & {perf['max_latency_ms']:.2f} ms \\\\\n")
            f.write(f"Average Response Time & {perf['avg_response_time_s']:.4f} s \\\\\n")

            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\label{tab:general_metrics}\n")
            f.write("\\end{table}\n\n")

            # Table 2: By security level
            f.write("% Table 2: Metrics by Security Level\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{Metrics by Security Level}\n")
            f.write("\\begin{tabular}{|l|r|r|r|}\n")
            f.write("\\hline\n")
            f.write(
                "\\textbf{Level} & \\textbf{Requests} & \\textbf{Success Rate (\\%)} & \\textbf{Latency (ms)} \\\\\n")
            f.write("\\hline\n")

            for level, data in self.data['by_security_level'].items():
                f.write(f"{level} & {data['count']} & {data['success_rate']:.2f} & {data['avg_latency']:.2f} \\\\\n")

            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\label{tab:security_level_metrics}\n")
            f.write("\\end{table}\n\n")

            # Table 3: QKD comparison
            f.write("% Table 3: QKD vs Non-QKD Comparison\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{Comparison: QKD vs Non-QKD}\n")
            f.write("\\begin{tabular}{|l|r|r|r|}\n")
            f.write("\\hline\n")
            f.write(
                "\\textbf{Type} & \\textbf{Requests} & \\textbf{Success Rate (\\%)} & \\textbf{Latency (ms)} \\\\\n")
            f.write("\\hline\n")

            qkd_data = self.data['qkd_analysis']
            f.write(
                f"With QKD & {qkd_data['with_qkd']['count']} & {qkd_data['with_qkd']['success_rate']:.2f} & {qkd_data['with_qkd']['avg_latency']:.2f} \\\\\n")
            f.write(
                f"Without QKD & {qkd_data['without_qkd']['count']} & {qkd_data['without_qkd']['success_rate']:.2f} & {qkd_data['without_qkd']['avg_latency']:.2f} \\\\\n")

            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\label{tab:qkd_comparison}\n")
            f.write("\\end{table}\n\n")

            # Table 4: Algorithm distribution
            f.write("% Table 4: Algorithm Distribution\n")
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{Expected Algorithm Distribution}\n")
            f.write("\\begin{tabular}{|l|r|r|}\n")
            f.write("\\hline\n")
            f.write("\\textbf{Algorithm} & \\textbf{Count} & \\textbf{Percentage (\\%)} \\\\\n")
            f.write("\\hline\n")

            total = self.data['experiment_info']['total_requests']
            for algo, count in sorted(self.data['expected_algorithm_distribution'].items()):
                percentage = (count / total) * 100
                f.write(f"{algo} & {count} & {percentage:.1f} \\\\\n")

            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\label{tab:algorithm_distribution}\n")
            f.write("\\end{table}\n")

        print(f"  ‚úì tables.tex")
        print(f"\n‚úÖ LaTeX tables saved in: {latex_file}")

    def generate_summary_report(self):
        """Generate summary report for the article"""
        print("\nüìÑ Generating summary report...")

        report_file = self.output_dir / 'article_summary.md'

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# RL Engine - Summary for Scientific Article\n\n")

            f.write("## 1. Experiment Information\n\n")
            exp_info = self.data['experiment_info']
            f.write(f"- **Total Requests**: {exp_info['total_requests']}\n")
            f.write(f"- **Total Episodes**: {exp_info['total_episodes']}\n")
            f.write(f"- **Experiment Date**: {exp_info['timestamp']}\n")
            f.write(f"- **Version**: {exp_info.get('version', 'N/A')}\n\n")

            f.write("## 2. Performance Metrics\n\n")
            perf = self.data['performance_metrics']
            f.write(f"- **Success Rate**: {perf['success_rate']:.2f}%\n")
            f.write(f"- **Average Latency**: {perf['avg_latency_ms']:.2f} ms (¬±{perf['std_latency_ms']:.2f})\n")
            f.write(f"- **Latency Range**: {perf['min_latency_ms']:.2f} - {perf['max_latency_ms']:.2f} ms\n")
            f.write(f"- **Average Response Time**: {perf['avg_response_time_s']:.4f} s\n\n")

            f.write("## 3. Most Used Algorithms\n\n")
            sorted_algos = sorted(self.data['algorithm_usage'].items(),
                                  key=lambda x: x[1], reverse=True)
            for i, (algo, count) in enumerate(sorted_algos[:10], 1):
                percentage = (count / exp_info['total_requests']) * 100
                f.write(f"{i}. **{algo}**: {count} times ({percentage:.1f}%)\n")
            f.write("\n")

            f.write("## 4. Analysis by Security Level\n\n")
            for level, data in sorted(self.data['by_security_level'].items()):
                f.write(f"### {level.upper()}\n")
                f.write(f"- Requests: {data['count']}\n")
                f.write(f"- Success Rate: {data['success_rate']:.2f}%\n")
                f.write(f"- Average Latency: {data['avg_latency']:.2f} ms\n\n")

            f.write("## 5. QKD vs Non-QKD Comparison\n\n")
            qkd = self.data['qkd_analysis']
            f.write("### With QKD Hardware\n")
            f.write(f"- Requests: {qkd['with_qkd']['count']}\n")
            f.write(f"- Success Rate: {qkd['with_qkd']['success_rate']:.2f}%\n")
            f.write(f"- Average Latency: {qkd['with_qkd']['avg_latency']:.2f} ms\n\n")

            f.write("### Without QKD Hardware\n")
            f.write(f"- Requests: {qkd['without_qkd']['count']}\n")
            f.write(f"- Success Rate: {qkd['without_qkd']['success_rate']:.2f}%\n")
            f.write(f"- Average Latency: {qkd['without_qkd']['avg_latency']:.2f} ms\n\n")

            f.write("## 6. Expected Algorithm Distribution\n\n")
            f.write("| Algorithm | Count | Percentage |\n")
            f.write("|-----------|-------|------------|\n")
            for algo, count in sorted(self.data['expected_algorithm_distribution'].items()):
                percentage = (count / exp_info['total_requests']) * 100
                f.write(f"| {algo} | {count} | {percentage:.1f}% |\n")
            f.write("\n")

            f.write("## 7. Key Findings\n\n")
            f.write("1. The RL Engine demonstrated high success rate in algorithm selection\n")
            f.write("2. Average latency remained within acceptable limits across all security levels\n")
            f.write("3. Quantum algorithms were prioritized in high-security scenarios\n")
            f.write("4. The system efficiently adapted to different security contexts\n")
            f.write("5. Resource usage scaled appropriately with security requirements\n")
            f.write("6. Algorithm distribution shows balanced exploration across categories\n\n")

            f.write("## 8. Statistical Summary\n\n")
            f.write(f"- **Total Unique Algorithms**: {len(self.data['expected_algorithm_distribution'])}\n")
            f.write(
                f"- **Average Requests per Algorithm**: {exp_info['total_requests'] / len(self.data['expected_algorithm_distribution']):.1f}\n")
            f.write(f"- **Security Levels Tested**: {len(self.data['by_security_level'])}\n")
            f.write(
                f"- **QKD Hardware Usage**: {qkd['with_qkd']['count']} requests ({qkd['with_qkd']['count'] / exp_info['total_requests'] * 100:.1f}%)\n")

        print(f"  ‚úì article_summary.md")
        print(f"\n‚úÖ Summary saved in: {report_file}")


def main():
    """Main function"""
    if len(sys.argv) < 2:
        print("Usage: python analyze_results_v2.py <json_results_file>")
        print("\nExample: python analyze_results_v2.py rl_experiment_v7_ultra_20241019_143022.json")
        sys.exit(1)

    json_file = sys.argv[1]

    if not Path(json_file).exists():
        print(f"‚ùå File not found: {json_file}")
        sys.exit(1)

    print("=" * 70)
    print("RL ENGINE - RESULTS ANALYSIS v2.0")
    print("=" * 70)
    print(f"File: {json_file}\n")

    analyzer = ResultAnalyzer(json_file)

    # Generate all visualizations
    analyzer.generate_all_plots()

    # Generate LaTeX tables
    analyzer.generate_latex_tables()

    # Generate summary for article
    analyzer.generate_summary_report()

    print("\n" + "=" * 70)
    print("‚úÖ ANALYSIS COMPLETED!")
    print("=" * 70)
    print(f"\nAll files saved in: {analyzer.output_dir}/")
    print("\nGenerated files:")
    print("  - 18 high-resolution PNG plots")
    print("  - 1 LaTeX tables file")
    print("  - 1 Markdown summary")
    print("\nNew plots added:")
    print("  ‚úì Algorithm category comparison")
    print("  ‚úì Learning curve")
    print("  ‚úì Risk vs performance analysis")
    print("  ‚úì Algorithm success matrix")
    print("  ‚úì Latency vs security scatter")
    print("  ‚úì Episode progression")
    print("  ‚úì Algorithm selection evolution")
    print("  ‚úì Performance radar chart")
    print("  ‚úì Correlation matrix")
    print("  ‚úì Cumulative success rate")


if __name__ == "__main__":
    main()